{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000b289b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_walk_ori\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrandom_walk\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlshashpy3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlshash\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graph'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import hashing.lsh_main as lsh\n",
    "# import graph.graph_main as graph\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import graph.random_walk_ori as random_walk\n",
    "import pandas as pd\n",
    "import lshashpy3 as lshash\n",
    "import math \n",
    "from scipy import stats\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "# data = moving_avg(data, 200).tolist()\n",
    "\n",
    "\n",
    "window = 3072\n",
    "\n",
    "nTS = 3\n",
    "\n",
    "percentage_reconst = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Electric.csv')\n",
    "data = data['Electric'].tolist()\n",
    "# df_segments = pd.read_csv('/localdata/ABench-IoT/Generation/gan/dcgan/fake_noise_23_raw_f3.txt', header = None)\n",
    "\n",
    "# data = pd.read_csv('/localdata/ABench-IoT/Generation/gan/dcgan_Sauerstoff/Sauerstoff.csv')\n",
    "# data = data['Sauerstoff'].tolist()\n",
    "data = stats.zscore(np.array(data))\n",
    "data = data.tolist()\n",
    "# data = data[:window * 6 ]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d391b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments = pd.read_csv('fake0.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd79b19",
   "metadata": {},
   "source": [
    "## Filter GAN generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "\n",
    "df_segments = df_segments.T\n",
    "\n",
    "n = 10  # the larger n is, the smoother curve will be\n",
    "b = [1.0 / n] * n\n",
    "a = 1\n",
    "\n",
    "for col in tqdm(df_segments):\n",
    "#     plt.plot(df_segments[col].tolist())\n",
    "    yy = lfilter(b,a,df_segments[col].tolist())\n",
    "    yy[:n] = yy[n:n+1]\n",
    "    df_segments[col] = yy.tolist()\n",
    "#     plt.plot(df_segments[col].tolist())\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "# df_segments = pd.DataFrame(segments)\n",
    "# df_segments = df_segments.T\n",
    "\n",
    "df_segments.iloc[: , :50].plot(subplots=True, layout=(10,6), figsize=(10, 10), legend = True, color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b98270",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [] \n",
    "\n",
    "for col in df_segments:\n",
    "    segments += [df_segments[col].tolist()]\n",
    "print(len(segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e7e07",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe694ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LSH(ori, generated, window, nTS, n_top = 30, hash_length = window // 300, num_hashtables=8):\n",
    "    # index synthetic segments\n",
    "    lsh = lshash.LSHash(hash_length, window, num_hashtables=num_hashtables)\n",
    "    for i in generated:\n",
    "        lsh.index(i) \n",
    "    to_query = [ori[i:i + window] for i in range(0, len(ori) - window, window)]\n",
    "    lsh_res = []\n",
    "    k = int(window * .03)\n",
    "    for i in tqdm(range(nTS)):\n",
    "        temp = list(random.choice( lsh.query(to_query[0], distance_func=\"euclidean\", num_results=n_top))[0][0])\n",
    "        for i in range(1, len(to_query)): \n",
    "            candidates = lsh.query(to_query[i], distance_func=\"euclidean\", num_results=n_top)\n",
    "            s = list(random.choice(candidates)[0][0])\n",
    "            temp_t = temp[-k:]\n",
    "            s_h = s[:k]\n",
    "            overlap = []\n",
    "            for i in range(-k//2, k//2):\n",
    "                overlap.append((1 - sigmoid(i))*temp_t[i + k//2] + sigmoid(i)*s_h[i + k//2])\n",
    "    #         for i in range(-1*k , 0):\n",
    "    #             temp[i]= (1 - sigmoid(i))*temp_t[i] + sigmoid(i)*s_h[i]\n",
    "    #         for i in range(0, k):\n",
    "    #             s[i]= (1 - sigmoid(i))*temp_t[i] + sigmoid(i)*s_h[i]\n",
    "            temp[-k:] = overlap\n",
    "            s[:k] = overlap[::-1]\n",
    "            temp.extend(s)\n",
    "        lsh_res.append(temp)\n",
    "    return lsh_res\n",
    "\n",
    "# lsh_res= LSH(data, segments, window, nTS)\n",
    "\n",
    "\n",
    "def LSH_update(ori, generated, window, nTS, n_top = 30, hash_length = window // 300, num_hashtables=8):\n",
    "    # index synthetic segments\n",
    "    lsh = lshash.LSHash(hash_length, window, num_hashtables=num_hashtables)\n",
    "    for i in generated:\n",
    "        lsh.index(i) \n",
    "    to_query = [ori[i:i + window] for i in range(0, len(ori) - window, window)]\n",
    "    lsh_res = []\n",
    "    used = 0\n",
    "    k = int(window * .03)\n",
    "    lsh_output = {}\n",
    "    start = time.time()\n",
    "    for i in tqdm(range(nTS)):\n",
    "        current = i \n",
    "        temp = list(random.choice( lsh.query(to_query[0], distance_func=\"euclidean\", num_results=n_top))[0][0])\n",
    "        for i in range(1, len(to_query)): \n",
    "            candidates = None\n",
    "            while candidates is None: \n",
    "                try:\n",
    "                    candidates = lsh.query(to_query[i], distance_func=\"euclidean\", num_results=n_top)\n",
    "                except:\n",
    "                     pass\n",
    "            s = list(random.choice(candidates)[0][0])\n",
    "#             temp_t = temp[-k:]\n",
    "#             s_h = s[:k]\n",
    "#             overlap = []\n",
    "#             for i in range(-k//2, k//2):\n",
    "#                 overlap.append((1 - sigmoid(i))*temp_t[i + k//2] + sigmoid(i)*s_h[i + k//2])\n",
    "#     #         for i in range(-1*k , 0):\n",
    "#     #             temp[i]= (1 - sigmoid(i))*temp_t[i] + sigmoid(i)*s_h[i]\n",
    "#     #         for i in range(0, k):\n",
    "#     #             s[i]= (1 - sigmoid(i))*temp_t[i] + sigmoid(i)*s_h[i]\n",
    "#             temp[-k:] = overlap\n",
    "#             s[:k] = overlap[::-1]\n",
    "            temp.extend(s)\n",
    "            used += 1\n",
    "        lsh_res.append(temp)\n",
    "#         print(used, len(generated)*50//100)\n",
    "        if used > len(generated) * percentage_reconst//100: \n",
    "            used = 0\n",
    "            lsh_output[current] = time.time() - start\n",
    "            print(lsh_output)\n",
    "            print('lsh reconstruction...')\n",
    "            lsh = lshash.LSHash(hash_length, window, num_hashtables=num_hashtables)\n",
    "            for i in generated:\n",
    "                lsh.index(i)    \n",
    "    return lsh_res, lsh_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d93e9",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the distance between two series. Given series A, B returns the Euclidean distance between A and B\n",
    "def distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "    \n",
    "#The probability is converted according to the sorted distances, which adds up to 1\n",
    "def distopro(a):\n",
    "    a=len(a)\n",
    "    if(a==3):\n",
    "        b=[0.2,0.3,0.5]\n",
    "    elif(a==4):\n",
    "        b=[0.1,0.2,0.3,0.4]\n",
    "    else:\n",
    "        b=[0.04,0.12,0.2,0.28,0.36]\n",
    "    return np.array(b)\n",
    "        \n",
    "\n",
    "#Input is the original data matrix, return is the relationship matrix relation_matrix, and probability matrix probability_matrix\n",
    "#Data is the matrix of series, the first dimension is the number of series, and the second dimension is each series\n",
    "#Window_size is the size of the window to calculate the distance, and k is the number of the nearest neighbors selected. Currently, 3,4,5 are supported\n",
    "def transform(data, window_size, k):\n",
    "    numOfSeq=data.shape[0]\n",
    "    distance_matrix=np.ones([numOfSeq,numOfSeq],dtype = float)\n",
    "    for i in range(numOfSeq):\n",
    "        for j in range(numOfSeq):\n",
    "            distance_matrix[i][j]=distance(data[i,data.shape[1]-window_size:],data[j,0:window_size])\n",
    "    relation_matrix=np.ones([numOfSeq,k],dtype = int)\n",
    "    subdistance_matrix=np.ones([numOfSeq,k],dtype = float)\n",
    "    probability_matrix=np.ones([numOfSeq,k],dtype = float)\n",
    "    for i in range(numOfSeq):\n",
    "        relation_matrix[i]=distance_matrix[i].argsort()[::-1][data.shape[0]-k:]\n",
    "        #print(relation_matrix[i])\n",
    "#     print(relation_matrix[i])\n",
    "    for i in range(numOfSeq):\n",
    "        for j in range(k):\n",
    "            subdistance_matrix[i][j]=distance_matrix[i][relation_matrix[i][j]]\n",
    "    \n",
    "    for i in range(numOfSeq):\n",
    "        probability_matrix[i]=distopro(subdistance_matrix[i])\n",
    "    \n",
    "    \n",
    "    return distance_matrix, subdistance_matrix ,relation_matrix, probability_matrix\n",
    "            \n",
    "      \n",
    "#print(transform(np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]]), 2, 3))\n",
    "\n",
    "\n",
    "#Given the ID of the current series, the ID of the next series is generated randomly according to probability\n",
    "def next_step(relation_array, probability_array):\n",
    "    value=random.random()\n",
    "#     print(value)\n",
    "    threshold=[0]\n",
    "    sum_value=0\n",
    "    for i in range(len(probability_array)):\n",
    "        sum_value=sum_value+probability_array[i]\n",
    "        threshold.append(sum_value)\n",
    "    for i in range(len(threshold)-1):\n",
    "        if(value>threshold[i] and value<=threshold[i+1]):\n",
    "            return relation_array[i]\n",
    "\n",
    "#Given a relation matrix and a probability matrix, returns a series of length        \n",
    "def random_walk(relation_matrix, probability_matrix, length):\n",
    "    seq=[0]\n",
    "    temp_id=0\n",
    "    for i in range(length-1):\n",
    "        temp_id=next_step(relation_matrix[temp_id],probability_matrix[temp_id])\n",
    "        seq.append(temp_id)\n",
    "        #print(temp_id)\n",
    "    return np.array(seq)\n",
    "\n",
    "\n",
    "def Graph(ori, generated, window, nTS):\n",
    "    a,b,c,d=transform(np.array(generated), 100, 5)\n",
    "    graph_res = []\n",
    "    for i in range(nTS):\n",
    "        path = random_walk( c, d, int(len(ori)/window))\n",
    "#         print(path)\n",
    "        temp=[]\n",
    "        for s in path:\n",
    "    #         print(path[i], i)\n",
    "            temp+=list(generated[s])\n",
    "        graph_res.append(temp)\n",
    "#     print(len(graph_res))   \n",
    "    return graph_res\n",
    "    \n",
    "def Graph_update(ori, generated, window, nTS):\n",
    "    used = 0\n",
    "    graph_res = []\n",
    "    graph_output = {}\n",
    "    start = time.time()\n",
    "    a,b,c,d=transform(np.array(generated), 100, 5)\n",
    "    for i in tqdm(range(nTS)):\n",
    "        currentTS = i\n",
    "        path = random_walk( c, d, int(len(ori)/window))\n",
    "#         print(path)\n",
    "        temp=[]\n",
    "        for s in path:\n",
    "    #         print(path[i], i)\n",
    "            temp+=list(generated[s])\n",
    "            used +=1\n",
    "        graph_res.append(temp)\n",
    "#         print(used, len(generated)*50//100)\n",
    "        if used > len(generated) * percentage_reconst//100: \n",
    "            used = 0\n",
    "            graph_output[currentTS] = time.time() - start\n",
    "            print(graph_output)\n",
    "            print('graph reconstruction...')\n",
    "            a,b,c,d=transform(np.array(generated), 100, 5)\n",
    "#         print(len(graph_res))   \n",
    "    return graph_res, graph_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26805273",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_seg = (4*segments)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_res = []\n",
    "# for i in range(nTS):\n",
    "#     path = random_walk( c, d, int(len(data)/window))\n",
    "#     print(path)\n",
    "#     temp=[]\n",
    "#     for s in path:\n",
    "# #         print(path[i], i)\n",
    "#         temp+=list(segments[s])\n",
    "#     graph_res.append(temp)\n",
    "#     print(len(graph_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410c024",
   "metadata": {},
   "source": [
    "# Experiment I - Construction Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_const = {}\n",
    "graph_const = {}\n",
    "for nSeg in tqdm(range(100, 1000 + 1 , 100)):\n",
    "    segments = long_seg[:nSeg]\n",
    "    # LSH \n",
    "    start = time.time()\n",
    "    lsh = lshash.LSHash(window // 300, 3072, num_hashtables=8)\n",
    "    for i in (segments):\n",
    "        lsh.index(i) \n",
    "    lsh_const[nSeg] = time.time() - start\n",
    "    \n",
    "    # Graph\n",
    "    start = time.time()\n",
    "    a,b,c,d=transform(np.array(segments), 100, 5)\n",
    "    graph_const[nSeg] = time.time() - start\n",
    "\n",
    "pd.DataFrame([lsh_const, graph_const]).T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56096af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_const, graph_const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0665977",
   "metadata": {},
   "source": [
    "# Experiment II - Input Segments Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61240e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsh_input = {}\n",
    "graph_input = {}\n",
    "for nSeg in tqdm(range(1000, 10000 + 1 , 1000)):\n",
    "    segments = long_seg[:nSeg]\n",
    "    # LSH \n",
    "    start = time.time()\n",
    "    lsh_res, _ = LSH_update(data, segments, window, nTS=10)\n",
    "    lsh_input[nSeg] = time.time() - start\n",
    "    # Graph\n",
    "    start = time.time()\n",
    "    graph_res, _ = Graph_update(data, segments, window, nTS=10)\n",
    "    graph_input[nSeg] = time.time() - start\n",
    "\n",
    "    print(lsh_input, graph_input)\n",
    "    pd.DataFrame([lsh_input, graph_input]).T.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsh_input, graph_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4013f",
   "metadata": {},
   "source": [
    "# Experiment III - Output Segments Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ec00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH \n",
    "\n",
    "lsh_res, lsh_output = LSH_update(data, long_seg, window, nTS=nTS, num_hashtables=3, n_top = 1)\n",
    "lsh_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa341279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "graph_res, graph_output = Graph_update(data, long_seg, window, nTS=nTS)\n",
    "graph_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsh_output, graph_output)\n",
    "\n",
    "pd.DataFrame([lsh_output, graph_output]).T.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef3d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
